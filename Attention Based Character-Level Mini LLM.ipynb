{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7uYsSUzUuIr5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpplUNTR93co",
    "outputId": "b4ebd6a9-fd28-469c-99ed-dea7e309820f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdeeff45250>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32                 # Number of sequences processed in one forward/backward pass\n",
    "block_size = 32               # Context window size (how many previous tokens to consider)\n",
    "n_embed = 256                  # Size of the token and positional embeddings\n",
    "n_layer = 6                   # Number of transformer blocks\n",
    "n_head = 8                    # Number of attention heads per transformer block\n",
    "dropout = 0.2                 # Dropout rate to reduce overfitting\n",
    "max_iters = 1000              # Total number of training iterations\n",
    "eval_iters = 100              # Number of batches used for loss evaluation\n",
    "eval_interval = 100           # Frequency of evaluation during training\n",
    "learning_rate = 3e-4          # Learning rate for the optimizer\n",
    "\n",
    "# Set the device for training (GPU if available)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set manual seed for reproducibility\n",
    "torch.manual_seed(1317)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpplUNTR93co",
    "outputId": "b4ebd6a9-fd28-469c-99ed-dea7e309820f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-01 04:05:33--  https://www.gutenberg.org/cache/epub/75854/pg75854.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 129993 (127K) [text/plain]\n",
      "Saving to: ‘pg75854.txt.1’\n",
      "\n",
      "pg75854.txt.1       100%[===================>] 126.95K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-05-01 04:05:33 (1.36 MB/s) - ‘pg75854.txt.1’ saved [129993/129993]\n",
      "\n",
      "--2025-05-01 04:05:34--  https://www.gutenberg.org/cache/epub/67709/pg67709.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42466 (41K) [text/plain]\n",
      "Saving to: ‘pg67709.txt.1’\n",
      "\n",
      "pg67709.txt.1       100%[===================>]  41.47K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-05-01 04:05:34 (1.25 MB/s) - ‘pg67709.txt.1’ saved [42466/42466]\n",
      "\n",
      "--2025-05-01 04:05:35--  https://www.gutenberg.org/cache/epub/75778/pg75778.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 161400 (158K) [text/plain]\n",
      "Saving to: ‘pg75778.txt.1’\n",
      "\n",
      "pg75778.txt.1       100%[===================>] 157.62K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-05-01 04:05:35 (1.77 MB/s) - ‘pg75778.txt.1’ saved [161400/161400]\n",
      "\n",
      "--2025-05-01 04:05:35--  https://www.gutenberg.org/cache/epub/75806/pg75806.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 447471 (437K) [text/plain]\n",
      "Saving to: ‘pg75806.txt.1’\n",
      "\n",
      "pg75806.txt.1       100%[===================>] 436.98K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-05-01 04:05:36 (3.65 MB/s) - ‘pg75806.txt.1’ saved [447471/447471]\n",
      "\n",
      "--2025-05-01 04:05:36--  https://www.gutenberg.org/cache/epub/75832/pg75832.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 162909 (159K) [text/plain]\n",
      "Saving to: ‘pg75832.txt.1’\n",
      "\n",
      "pg75832.txt.1       100%[===================>] 159.09K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-05-01 04:05:36 (1.77 MB/s) - ‘pg75832.txt.1’ saved [162909/162909]\n",
      "\n",
      "--2025-05-01 04:05:37--  https://www.gutenberg.org/cache/epub/75766/pg75766.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 268305 (262K) [text/plain]\n",
      "Saving to: ‘pg75766.txt.1’\n",
      "\n",
      "pg75766.txt.1       100%[===================>] 262.02K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-05-01 04:05:37 (2.22 MB/s) - ‘pg75766.txt.1’ saved [268305/268305]\n",
      "\n",
      "--2025-05-01 04:05:38--  https://www.gutenberg.org/cache/epub/75870/pg75870.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 48716 (48K) [text/plain]\n",
      "Saving to: ‘pg75870.txt.1’\n",
      "\n",
      "pg75870.txt.1       100%[===================>]  47.57K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-05-01 04:05:38 (1.55 MB/s) - ‘pg75870.txt.1’ saved [48716/48716]\n",
      "\n",
      "--2025-05-01 04:05:38--  https://www.gutenberg.org/cache/epub/75948/pg75948.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1472767 (1.4M) [text/plain]\n",
      "Saving to: ‘pg75948.txt.1’\n",
      "\n",
      "pg75948.txt.1       100%[===================>]   1.40M  8.10MB/s    in 0.2s    \n",
      "\n",
      "2025-05-01 04:05:39 (8.10 MB/s) - ‘pg75948.txt.1’ saved [1472767/1472767]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.gutenberg.org/cache/epub/75854/pg75854.txt\n",
    "!wget https://www.gutenberg.org/cache/epub/67709/pg67709.txt\n",
    "!wget https://www.gutenberg.org/cache/epub/75778/pg75778.txt\n",
    "!wget https://www.gutenberg.org/cache/epub/75806/pg75806.txt\n",
    "!wget https://www.gutenberg.org/cache/epub/75832/pg75832.txt\n",
    "!wget https://www.gutenberg.org/cache/epub/75766/pg75766.txt\n",
    "!wget https://www.gutenberg.org/cache/epub/75870/pg75870.txt\n",
    "!wget https://www.gutenberg.org/cache/epub/75948/pg75948.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMNIhWUZ9X5v",
    "outputId": "bb6967dd-2756-4631-9d40-deb4d771922b"
   },
   "outputs": [],
   "source": [
    "files = [\n",
    "    'pg75854.txt',\n",
    "    'pg67709.txt',\n",
    "    'pg75778.txt',\n",
    "    'pg75806.txt',\n",
    "    'pg75832.txt',\n",
    "    'pg75766.txt',\n",
    "    'pg75870.txt',\n",
    "    'pg75948.txt',\n",
    "    'pg75854.txt',\n",
    "    'pg67709.txt',\n",
    "    'pg75778.txt',\n",
    "    'pg75806.txt',\n",
    "    'pg75832.txt',\n",
    "    'pg75766.txt',\n",
    "    'pg75870.txt',\n",
    "    'pg75948.txt'\n",
    "]\n",
    "\n",
    "\n",
    "text = \"\"\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding = 'utf-8') as f:\n",
    "        text += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMNIhWUZ9X5v",
    "outputId": "bb6967dd-2756-4631-9d40-deb4d771922b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = text.lower()\n",
    "text = re.sub('[^a-z0-9 ]', '', text)\n",
    "text = re.sub('/s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGa_rfnh8v2d",
    "outputId": "f5eb080a-5e74-49c2-8eae-36ec50a0ed9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "\n",
    "# Create a mapping from characters to integer\n",
    "ctoi = {char:index for index,char in enumerate(chars)}\n",
    "itoc = {index:char for index,char in enumerate(chars)}\n",
    "encode = lambda s: [ctoi[char] for char in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itoc[index] for index in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "R1HhoIHd9M-v"
   },
   "outputs": [],
   "source": [
    "# Train and validation splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5077320"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total nos of characters in data set\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Pn_qSyBU9N-4"
   },
   "outputs": [],
   "source": [
    "# Function to generate a batch of input-target pairs for training or validation\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Randomly choose starting indices for each sequence in the batch\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Create input sequences (x) and target sequences (y) shifted by one position\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
    "\n",
    "    # Move data to the appropriate device (CPU or GPU)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3Ce4OTB99Q_d"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "# Function to evaluate the model and report the loss\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DQ15GrT7_mDF"
   },
   "outputs": [],
   "source": [
    "# One head of self-attention (used inside a multi-head attention block)\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layers to compute key, query, and value vectors without biases\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        # Lower triangular matrix for causal masking (prevent attention to future tokens)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout to regularize attention weights\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # B: batch size, T: sequence length, C: embedding size\n",
    "\n",
    "        # Compute keys and queries\n",
    "        k = self.key(x)        # (B, T, head_size)\n",
    "        q = self.query(x)      # (B, T, head_size)\n",
    "\n",
    "        # Compute scaled dot-product attention scores\n",
    "        attention = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n",
    "\n",
    "        # Apply causal mask to prevent attending to future tokens\n",
    "        attention = attention.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # Normalize attention weights and apply dropout\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Compute values and apply attention weights\n",
    "        v = self.value(x)      # (B, T, head_size)\n",
    "        out = attention @ v    # (B, T, head_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MdRqgt4RBjXd"
   },
   "outputs": [],
   "source": [
    "# Multiple heads of self-attention in parallel\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embed, n_embed)  #Wq, Wk, Wv\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yTJy6zbGC9o6"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_head * n_embed),  # 512 x 2048\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_head * n_embed, n_embed), # 2048 x 512\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "v-UXDQUVEArm"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):   # Block - Encoder Layer\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head    # 512//8 = 64\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)   # 8, 64\n",
    "        self.ffwd = FeedFoward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = x + self.sa(self.ln1(x))   # Residual connections\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCcRSb1xFIVS",
    "outputId": "598acf51-f92f-461a-db9a-a303615713ec"
   },
   "outputs": [],
   "source": [
    "class MyGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedding layer: maps each token ID to a vector of size n_embed\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # Positional embedding: provides position-specific context to each token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        # Stack of transformer blocks (self-attention + feedforward layers)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "        # Final layer normalization before output\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "\n",
    "        # Output layer: projects back to vocabulary size for logits\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape  # Batch size and sequence length\n",
    "\n",
    "        # Token and positional embeddings (B,T,C)\n",
    "        tok_embed = self.token_embedding_table(index)\n",
    "        pos_embed = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_embed + pos_embed\n",
    "\n",
    "        # Pass through transformer blocks and final layer norm\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Compute logits for next token prediction\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # If targets are provided, compute cross-entropy loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # Generate new tokens iteratively\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Use only the last block_size tokens as context\n",
    "            context_window = index[:, -block_size:]\n",
    "\n",
    "            # Forward pass to get logits for the next token\n",
    "            logits, loss = self(context_window)\n",
    "            logits = logits[:, -1, :]  # Focus on the last time step\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append sampled token to the sequence\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4jezhywHPmz",
    "outputId": "1eb7de04-6994-477b-bb44-fb36187dc171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.913509 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = MyGPT()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWV1q_Z-Hbw4",
    "outputId": "7ecaf550-27fb-4d6a-8bb4-c48efa44e1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 3.8063, val loss 3.8119\n",
      "Step 100: train loss 2.4048, val loss 2.4587\n",
      "Step 200: train loss 2.3225, val loss 2.3889\n",
      "Step 300: train loss 2.1819, val loss 2.2317\n",
      "Step 400: train loss 2.0950, val loss 2.1248\n",
      "Step 500: train loss 2.0342, val loss 2.0658\n",
      "Step 600: train loss 1.9856, val loss 1.9994\n",
      "Step 700: train loss 1.9407, val loss 1.9653\n",
      "Step 800: train loss 1.9042, val loss 1.9285\n",
      "Step 900: train loss 1.8758, val loss 1.8932\n",
      "Step 999: train loss 1.8541, val loss 1.8587\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AdamW optimizer with model parameters and specified learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # Evaluate and print training/validation loss at regular intervals\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Get a batch of training data (inputs and targets)\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "\n",
    "    # Forward pass: compute predictions and loss\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "\n",
    "    # Backward pass and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear gradients\n",
    "    loss.backward()                        # Backpropagate the loss\n",
    "    optimizer.step()                       # Update model parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYXIkFbaINdK",
    "outputId": "310a4065-672b-4cf5-91f2-18fd3490e434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " unde   hist alongrausteng aroned whatles with casneer on the   chantima to sheele of than in spexic when is soecculded                gending to deain shaprvybuch had it as way beeare a hue asy must lestowng rame we arnd do bery are caintle father was napte hemady it markan cen be hough dain was an didove diacking lates and reverycummalandy so ariung dy mower fal deeppaine butchacil is all a but ut a gexisuppentere why he gual conners the clay the un of thebegenatog 52 witherls arere earn i usybeir upply whh evemby with uxher posts coerfable dayter foll by abbean and and heow the wourk for whiling sa myeng hous go and maketh sime and from evening of achish the selatureesh it   a foughtread to more gaintard grending out use and butage theanunnere pmawing a nated in froms a tranginemmothere head  greadelw to that up t2     with by pelughtly grainned e7pongivel thas in therve prijust the fach to nowss plenices the regare at goend ten earchiangs in i campomme afcure sawe hing from rest inl\n"
     ]
    }
   ],
   "source": [
    "# Start generation with a single token (index 0) on the correct device\n",
    "firstindex = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# Generate 2000 new tokens from the model and decode them back into readable text\n",
    "print(decode(model.generate(index=firstindex, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrrANrR3ImWd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
