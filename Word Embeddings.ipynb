{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692fd086-b1b7-4b25-bedb-5949c7563711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire news corpus text file into a single string\n",
    "with open ('news_corpus.txt', 'r') as file:\n",
    "  content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d56e5799-c220-496b-98c4-ef85460225bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries: numpy and pandas for later use\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36349940-415b-43a0-94d1-55357171860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "906868ea-965f-40c3-af77-655995031683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the simple_preprocess tokenizer from Gensim for word-level tokenization\n",
    "from gensim.utils import simple_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "239380b1-1f8f-41bf-9636-5c0148e38966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text by splitting it into sentences using '.' and tokenizing each sentence into words\n",
    "# Only include sentences with more than 2 characters\n",
    "\n",
    "raw_text = content.split('.')\n",
    "\n",
    "text = []\n",
    "for sentence in raw_text:\n",
    "  if len(sentence) > 2:\n",
    "    text.append(list(simple_tokenize(sentence.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b14ec5-bcaf-482d-9e4f-4593732f08c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Current', 'events', 'of', 'September', 'Sunday', 'eBay', 'is', 'founded']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first tokenized sentence to verify preprocessing\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca5b0439-4dda-43da-b117-28ffc7cf901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Word2Vec model with specified parameters:\n",
    "# window: context window size\n",
    "# min_count: ignore words with total frequency lower than 3\n",
    "# vector_size: dimensionality of word vectors\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=5,   # We will take only those sentences which have a min 3 words\n",
    "    vector_size=100 # Representation vector size for each word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6bd46cf-5c67-4e0b-a394-d7e7b84964d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary from the preprocessed tokenized text\n",
    "model.build_vocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "005afff9-6d37-4b39-8085-74f2bc78596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7614998, 9863760)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Word2Vec model on the tokenized corpus\n",
    "model.train(text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ae4082c-8362-492d-874a-63d1f42d24e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.095347494"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity score between the words \"woman\" and \"man\"\n",
    "model.wv.similarity('water', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0399140d-de94-43cb-a3c9-9930d691a753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93971"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808e4ec-9232-4aa2-85d9-44344a80a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bada6fe-5a58-43df-8a1f-d63f15601155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
